<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset='utf-8'>
	<meta name='viewport' content='width=device-width,initial-scale=1'>

	<script src="https://distill.pub/template.v2.js"></script>
	<script src="https://unpkg.com/d3@5.15.0/dist/d3.min.js"></script>
	<script src="https://unpkg.com/lodash@1.0.2/dist/lodash.min.js"></script>

	<title>Svelte app</title>

	<link rel='icon' type='image/png' href='/favicon.png'>
	<link rel='stylesheet' href='/global.css'>
	<link rel='stylesheet' href='/build/bundle.css'>

	<script defer src='/build/bundle.js'></script>

	<style>
		html {
			scroll-padding-top: 80px;
		}

		.token {
			unicode-bidi: embed;
			font-family: monospace;
			/* color: #B91C1C; */
			color: #D97706;
			padding: 0rem 0.2rem;
		}

		.token-chunk {
			/* quotes: "“" "”" "‘" "’"; */
			font-weight: 500;
			/* font-style: italic; */
		}

		.token-chunk::before {
			content: open-quote;
		}

		.token-chunk::after {
			content: close-quote;
		}

		.flyvec {
			font-variant: small-caps;
			/* text */
		}

		.neuron {
			font-weight: 600;
			color: #059669;
		}
	</style>
</head>

<body>
	<div id="nav-container"></div>
	<div id="main-demo"></div>

	<hr />

	<d-front-matter>
		<script id='distill-front-matter' type="text/json">
			{
				"title": "Can a Fruit Fly Learn Word Embeddings?",
				"description": "Creating sparse and binary word vectors using algorithms inspired by the fruit fly's olfactory circuit.",
				"authors": [
					{
						"author":"Benjamin Hoover",
						"authorURL":"https://bhoov.com",
						"affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
					},
					{
						"author":"Yuchen Liang",
						"authorURL":"https://airc.rpi.edu/people/staff/yuchen-liang",
						"affiliations": [
							{"name": "RPI", "url": "https://science.rpi.edu/computer-science"},
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"}
						]
					},
					{
						"author":"Chaitanya K. Ryali",
						"authorURL":"https://scholar.google.com/citations?user=4LWx24UAAAAJ&hl=en",
						"affiliations": [{"name": "UC San Diego", "url": "https://cse.ucsd.edu/"}]
					},
					{
						"author":"Leopold Grinberg",
						"authorURL":"http://www.dam.brown.edu/people/lgrinb/",
						"affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
					},
					{
						"author":"Hendrik Strobelt",
						"authorURL":"http://hendrik.strobelt.com/",
						"affiliations": [
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"},
							{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}
						]
					},
					{
						"author":"Saket Navlakha",
						"authorURL":"https://navlakhalab.net/",
						"affiliations": [{"name": "Cold Spring Harbor Laboratory", "url": "https://www.cshl.edu/"}]
					},
					{
						"author":"Mohammed J. Zaki",
						"authorURL":"http://www.cs.rpi.edu/~zaki/",
						"affiliations": [{"name": "RPI", "url": "https://science.rpi.edu/computer-science"}]
					},
					{
						"author":"Dmitry Krotov",
						"authorURL":"https://mitibmwatsonailab.mit.edu/people/dmitry-krotov/",
						"affiliations": [
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"},
							{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}
						]
					}
				],
				"katex": {
					"delimiters": [
						{"left": "$", "right": "$", "display": false}
					]
				}
			}
		  </script>
	</d-front-matter>

	<d-title>
		<h1 id="blog-title">Can a Fruit Fly Learn Word Embeddings?</h1>
		<!-- <figure class="l-body" style="margin-top: 2vh; margin-bottom: 2vh;">
			<img src="./assets/img/teaser.svg" style="width:100%;">
		</figure> -->
	</d-title>
	<d-byline class="hidden"></d-byline>

	<d-article>
		<p>
			In <a href="https://arxiv.org/abs/2101.06887" target="_blank" rel="noopener noreferrer">this work</a>,
			accepted to ICLR 2021, we study a
			well-established neurobiological network motif from the fruit fly brain and investigate the possibility of
			reusing its architecture for solving common natural language processing (NLP) tasks. Specifically, we focus
			on the network of <a href="https://en.wikipedia.org/wiki/Kenyon_cell" target="_blank"
				rel="noopener noreferrer">Kenyon cells (KCs)</a> in the <a
				href="https://en.wikipedia.org/wiki/Mushroom_bodies" target="_blank" rel="noopener noreferrer">mushroom
				body</a> of the fruit fly brain. This
			network was trained on a large corpus of text and tested on common NLP tasks. Biologically, this network
			evolved to process sensory stimuli from olfaction, vision, and other modalities and not to understand
			language. It, however, uses a general-purpose algorithm to embed inputs
			(from different modalities) into a high dimensional space so that these embeddings are locality sensitive
			<d-cite key="dasgupta_neural_2017, BioHash"></d-cite>.
			This means that similar inputs are mapped to similar representations, and different inputs are mapped to
			distant representations.
			Thus, our work illustrates a theoretical possibility to <strong>reprogram a biological, general-purpose
				network</strong> to solve a useful machine learning task that the biological organism does not naturally
			engage in.
		</p>

		<p>
			In our network, the semantic meanings of words and their textual contexts are encoded in
			the pattern of activations of the KCs. It is convenient to represent this pattern of activations as a binary
			vector, where the active KCs are assigned the state 1 and the inactive KCs are assigned the state 0. Thus,
			our
			word embeddings, which we call <span class="flyvec font-bold">FlyVec</span>, are represented by boolean,
			logical
			vectors, in contrast to conventional continuous word embeddings like <a
				href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener noreferrer">Word2vec</a><d-cite key="mikolov2013efficient"></d-cite> or <a href="https://nlp.stanford.edu/projects/glove/"
				target="_blank" rel="noopener noreferrer">GloVe</a><d-cite key="pennington2014glove"></d-cite>. In this
			blogpost, we explore the concepts learned by the individual neurons in this network. We also study patterns of a neuron’s excitations in response to a query
			sentence.
		</p>

		<h2 id="blog-biological-network">
			Biological Network
		</h2>

		<p>
			The mushroom body is a major area of the brain that is responsible for processing sensory information in
			fruit
			flies. It receives inputs from a set of projection neurons (PN) conveying information from several sensory
			modalities. The major modality is olfaction<d-cite key="bates2020complete"></d-cite>, but there are also
			inputs from the PNs that send temperature, humidity<d-cite key="marin2020connectomics">
			</d-cite>, and visual inputs<d-cite key="vogt2016direct, caron2020two"></d-cite>. The sensory signals
			are forwarded to a population of Kenyon cells through a set of synaptic weights. KCs are reciprocally
			connected through an anterior paired lateral (APL) neuron, which sends a strong inhibitory signal back to
			KCs. This recurrent network implements a winner-takes-all competition between the KCs, and silences all but
			a small fraction of top activated neurons. The schematic of this architectural motif is shown in the figure
			below. In fruit flies, KCs also send their outputs to mushroom body output neurons, but this part of the
			network is not included in our mathematical model.

		</p>

		<d-figure>
			<figure id="fig-network-architecture" class="l-body-outset">
				<img src="./img/architecture.001.png" style="width: 100%" class="mb-2" />
				<figcaption>
					Network architecture. Several groups of PNs corresponding to different modalities send
					their activities to the layer of KCs, which are inhibited through the reciprocal connections to the
					APL neuron.
				</figcaption>
			</figure>
		</d-figure>

		<h2 id="blog-training">
			Training the Network
		</h2>


		<p>
			We decomposed each sentence from the training corpus into a collection of $w$-grams that represent
			consecutive words.
			The input to the network is a vector that has two blocks: <strong>context</strong> and
			<strong>target</strong>. The target block is a one-hot
			encoding of the middle token in the $w$-gram, and the remaining tokens in the $w$-gram are placed in the
			context block as a bag of tokens.
			An unsupervised learning algorithm<d-cite key="krotovHopfield2019"></d-cite> is used
			to learn the weights of the neural network (see also <a
				href="https://www.ibm.com/blogs/research/2019/04/biological-algorithm/" target="_blank"
				rel="noopener noreferrer">this blog post</a>). Mapping
			this idea back to the network's biological inspiration in fruit flies, we can think about the context as an
			olfactory input to the network of KCs and the target word
			as a visual input. An example of the $w$-gram encoding is shown below.
		</p>

		<d-figure>
			<figure class="l-body">
				<img src="./img/ENCODING.003.png" style="width: 100%" />
				<figcaption>
					The encoding method. The input vector consists of two blocks separated by the (thick) blue
					line. Assuming $w = 3$ and the center word <span class="token">stock</span> is the target word, the
					two flanking words <span class="token">Apple</span> and <span class="token">rises</span> form a
					context. The trigram is highlighted in light blue.
				</figcaption>
			</figure>
		</d-figure>

		<h2 id="blog-explorer">
			Individual KCs Explorer
		</h2>

		<p>
			In the demo, the user can select individual neurons from the group of KCs and explore the strength of
			synaptic weights connecting the selected KC to the PNs. Each PN corresponds to a token from our vocabulary.
			The larger the synaptic weight, the stronger that token contributes to the excitation signal to the given KC
			and the bigger (relatively) it appears in that KC's word-cloud. For every KC, the strengths of incoming
			synapses from the PNs are passed through a softmax function and the twenty strongest inputs are additionally
			displayed as a histogram.
		</p>

		<p>
			Consider <span class="neuron">Neuron 75</span>. The top three tokens activating this KC correspond to the
			words: <span class="token">disease</span>, <span class="token">disorder</span>, <span
				class="token">patients</span>.
			It is clear that this neuron has learned a concept associated with medical conditions.
		</p>

		<figure>
			<div id="fig-neuron-disease" style="height: 380px;"></div>
		</figure>

		<p>
			Another example, <span class="neuron">Neuron 353</span> has the top tokens: <span
				class="token">governments</span>, <span class="token">authorities</span>, <span
				class="token">state</span>, as shown below. Thus, it has learned
			a concept of authorities of various levels. It is clear, however, that this concept is not pure. For
			example, the fifth strongest token here is <span class="token"><code>&lt;NUM&gt;</code></span> which
			represents
			a number-token. Perhaps this is not surprising given that a number token can easily
			appear in the training corpus quantifying, for example, a number of law enforcement officers. Another sign
			of entanglement of concepts for this KC is the token <span class="token">news</span>. While it is not a
			systemic authority, this word can easily appear in the context of government since this topic is often
			discussed in the news.
		</p>

		<d-figure>
			<figure>
				<div id="fig-neuron-authority" style="height:380px;"></div>
			</figure>
		</d-figure>

		<!-- <p>
			This demo allows the user to select every individual neuron in this network and inspect the synapses that
			this neuron has learned.
		</p> -->

		<h2 id="blog-query">
			Response to a Query Sentence
		</h2>

		<p>
			In the second part of the demo, we explore the activation patterns of neurons in the fruit fly network in
			response to an input sentence. Our model encodes the query sentence as a bag of words and interprets it as
			the activity of PNs. The recurrent network of KCs coupled to the APL neuron performs the calculation and
			outputs a pattern of excitations across KCs so that only a small number of KCs are in the ON state, while
			the majority of them are in the OFF state. This pattern of excitations is the hash code for the query
			sentence, which is used for the evaluations of our algorithm (see the <a
				href="https://arxiv.org/abs/2101.06887" target="_blank" rel="noopener noreferrer">paper</a>).
		</p>

		<p>
			For a given input sentence, our demo returns the word-cloud visualizations of words strongly contributing to
			the excitation of the four top activated KCs. The synaptic weights learned by these KCs are used to generate
			the probability distributions of individual tokens by passing the weights through a softmax function. These
			probability distributions are visualized as word-clouds.
		</p>

		<p>
			Consider the example sentence:
		</p>

		<blockquote class="">Entertainment industry shares rise following the premiere of the mass
			destruction weapon documentary.</blockquote>

		<d-figure>
			<figure>
				<div id="fig-entertainment-cloud"
					class="grid grid-cols-2 lg:grid-flow-row lg:grid-cols-4 place-items-center"
					style="column-gap: 0.075rem"></div>
				<figcaption>
					The top results for the word-clouds of tokens learned by the four highest activated KCs. In the
					inset of each word-cloud, one can see the strength of activation for that specific KC (highlighted
					in red) compared to the activations of the remaining KCs (shown in gray).
				</figcaption>
			</figure>
		</d-figure>

		<p>
			This query sentence activates several KCs. The demo shows the word clouds of tokens learned by the four
			highest-activated KCs. In the inset of each word cloud, one can see the strength of the activation of that
			specific KC (highlighted in red) compared to the activations of the remaining KCs (shown in gray). The top
			activated KC has the largest weights for the words: <span class="token">weapon</span>, <span
				class="token">mass</span>, etc. The second-highest activated KC is sensitive to the words <span
				class="token">market</span>, <span class="token">stock</span>, etc. This illustrates how the fruit fly
			network processes the queries. In this example, the query refers to several distinct combinations of
			concepts: <span class="token-chunk">weapon of mass destruction</span>, <span class="token-chunk">stock
				market</span>, <span class="token-chunk">movie industry</span>. Each of those concepts has a dedicated
			KC responsible for it. However, the responses are not perfect. In this case, we would expect to have the KC
			which is responsible for the <span class="token-chunk">movie industry</span> concept to have a higher
			activation than the KC responsible for the types of <span class="token-chunk">weapons of mass
				destruction</span>. But, overall, all the concepts picked by the KCs are meaningful and related to the
			query.
		</p>

		<p>
			The user can query the fruit fly network with different sentences. We have observed that our network works
			best on queries that are taken from news articles (preferably with political or financial content).
			Presumably, this is a result of the particular corpus that was used for training. We have included both good
			examples (denoted by ✅ ) where the responses of the network are reasonable, as well as bad examples (denoted
			by ❌ ) where the network fails to identify the relevant concepts.
		</p>

		<h2>Summary</h2>

		<p>
			In this work, we asked the intriguing question of whether the core computational algorithm of the network of
			KCS in the fruit fly brain &mdash; one of the best studied networks in neuroscience &mdash; can be
			repurposed for learning word embeddings from text, which is a well defined machine learning task. We have
			shown that, surprisingly, this network can learn correlations between words and their context while
			producing high quality word embeddings. Our fruit fly network generates binary word embeddings &mdash;
			vectors of <code>[0,1]</code> as opposed to continuous vectors like GloVe and Word2vec &mdash; which is
			useful from the perspective of memory efficiency. The boolean nature of the <span
				class="flyvec">FlyVec</span> word embeddings also raises interesting questions related to
			interpretability of the learned embeddings. As we have illustrated in this demo, some of the individual KCs
			have learned semantically coherent concepts. For example, we could ask the question, “Does the query
			sentence pertain to a medical condition?”. If the answer is positive, it is likely that the <span
				class="neuron">Neuron 75</span>,
			discussed above, will be in the ON state for that query. Thus, individual KCs may encode a logical way to
			detect the presence or the absence of certain attributes of the words and query sentences. We hope to
			systematically investigate the intriguing interpretability of <span class="flyvec">FlyVec</span> embeddings
			in the future.
		</p>

		<hr class="w-full" />
		<div id="blog-authors"></div>

	</d-article>



	<d-appendix>
		<d-bibliography src="/bibliography.bib"></d-bibliography>
	</d-appendix>
</body>

</html>