<html lang="en">

<head>
	<meta charset='utf-8'>
	<meta name='viewport' content='width=device-width,initial-scale=1'>

	<script src="https://distill.pub/template.v2.js"></script>

	<title>Svelte app</title>

	<link rel='icon' type='image/png' href='/favicon.png'>
	<script defer src="https://use.fontawesome.com/releases/v5.15.2/js/all.js"
		integrity="sha384-vuFJ2JiSdUpXLKGK+tDteQZBqNlMwAjhZ3TvPaDfN9QmbPb7Q8qUpbSNapQev3YF"
		crossorigin="anonymous"></script>
	<link rel='stylesheet' href='/global.css'>
	<link rel='stylesheet' href='/build/vendor.css'>
	<link rel='stylesheet' href='/build/bundle.css'>

	<script defer src='/build/bundle.js'></script>

	<style>
		html {
			scroll-padding-top: 80px;
		}

		.token {
			unicode-bidi: embed;
			font-family: monospace;
			color: #B91C1C;
			padding: 0rem 0.2rem;
		}

		.token-chunk {
			/* quotes: "“" "”" "‘" "’"; */
			font-weight: 500;
			/* font-style: italic; */
		}

		.token-chunk::before {
			content: open-quote;
		}

		.token-chunk::after {
			content: close-quote;
		}
	</style>
</head>

<body>
	<div id="nav-container"></div>
	<div id="main-demo"></div>

	<hr/>

	<d-front-matter>
		<script id='distill-front-matter' type="text/json">
			{
				"title": "Can a Fruit Fly Learn Word Embeddings?",
				"description": "Creating sparse and binary word vectors, inspired by the fruit fly's olfactory circuit.",
				"authors": [
					{
						"author":"Benjamin Hoover",
						"authorURL":"https://bhoov.com",
						"affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
					},
					{
						"author":"Yuchen Liang",
						"authorURL":"https://airc.rpi.edu/people/staff/yuchen-liang",
						"affiliations": [
							{"name": "RPI", "url": "https://science.rpi.edu/computer-science"},
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"}
						]
					},
					{
						"author":"Chaitanya K. Ryali",
						"authorURL":"https://scholar.google.com/citations?user=4LWx24UAAAAJ&hl=en",
						"affiliations": [{"name": "UC San Diego", "url": "https://cse.ucsd.edu/"}]
					},
					{
						"author":"Leopold Grinberg",
						"authorURL":"http://www.dam.brown.edu/people/lgrinb/",
						"affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
					},
					{
						"author":"Hendrik Strobelt",
						"authorURL":"http://hendrik.strobelt.com/",
						"affiliations": [
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"},
							{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}
						]
					},
					{
						"author":"Saket Navlakha",
						"authorURL":"https://navlakhalab.net/",
						"affiliations": [{"name": "Cold Spring Harbor Laboratory", "url": "https://www.cshl.edu/"}]
					},
					{
						"author":"Mohammed J. Zaki",
						"authorURL":"http://www.cs.rpi.edu/~zaki/",
						"affiliations": [{"name": "RPI", "url": "https://science.rpi.edu/computer-science"}]
					},
					{
						"author":"Dmitry Krotov",
						"authorURL":"https://mitibmwatsonailab.mit.edu/people/dmitry-krotov/",
						"affiliations": [
							{"name": "MIT-IBM Watson AI Lab", "url": "https://mitibmwatsonailab.mit.edu/"},
							{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}
						]
					}
				],
				"katex": {
					"delimiters": [
						{"left": "$", "right": "$", "display": false}
					]
				}
			}
		  </script>
	</d-front-matter>


	<d-title>
		<h1 id="blog-title">Can a Fruit Fly Learn Word Embeddings?</h1>
		<!-- <figure class="l-body" style="margin-top: 2vh; margin-bottom: 2vh;">
			<img src="./assets/img/teaser.svg" style="width:100%;">
		</figure> -->
	</d-title>

	<d-article>
		<p>
			In <a href="https://arxiv.org/abs/2101.06887" target="_blank" rel="noopener noreferrer">this work</a>, accepted to ICLR 2021, we study a
			well-established neurobiological network motif from the fruit fly brain and investigate the possibility of
			reusing its architecture for solving common natural language processing (NLP) tasks. Specifically, we focus
			on the network of <a href="https://en.wikipedia.org/wiki/Kenyon_cell" target="_blank" rel="noopener noreferrer">Kenyon cells (KCs)</a> in the <a
				href="https://en.wikipedia.org/wiki/Mushroom_bodies" target="_blank" rel="noopener noreferrer">mushroom body</a> of the fruit fly brain. This
			network was trained on a large corpus of text and tested on common NLP tasks. Biologically, this network has
			evolved to process sensory stimuli from olfaction, visual system, and other modalities and not to <span
				class="token">understand</span> language. It, however, uses a general purpose algorithm to embed inputs
			(from different modalities) into a high dimensional space, so that these embeddings are locality sensitive.<d-cite key="dasgupta_neural_2017, BioHash"></d-cite> 
			This means that inputs that are similar to each other
			are mapped to similar representations, and inputs that are different are mapped to distant representations.
			Thus, our work illustrates a theoretical possibility to <strong>reprogram a biological general-purpose
				network</strong> to solve a useful machine learning task that the biological organism does not
			necessarily naturally engage in.
		</p>

		<p>
			In this blogpost we explore the concepts learned by the individual neurons of this network. We also study
			patterns of neuron’s excitations in response to a query sentence.
			<!-- The pretrained model is available at (link). -->
		</p>

		<h2 id="blog-biological-network">
			Biological Network
		</h2>

		<p>

			The mushroom body is a major area of the brain responsible for processing sensory information in fruit
			flies. It receives inputs from a set of projection neurons (PN) conveying information from several sensory
			modalities. The major modality is olfaction,<d-cite key="bates2020complete"></d-cite> but there are also
			inputs from the PNs responsible for sensing temperature and humidity,<d-cite key="marin2020connectomics">
			</d-cite> as well as visual inputs.<d-cite key="vogt2016direct, caron2020two"></d-cite> The sensory signals
			are forwarded to a population of Kenyon cells through a set of synaptic weights. KCs are reciprocally
			connected through an anterior paired lateral (APL) neuron, which sends a strong inhibitory signal back to
			KCs. This recurrent network implements a winner-takes-all competition between the KCs, and silences all but
			a small fraction of top activated neurons. The schematic of this architectural motif is shown in the figure
			below. In fruit flies, KCs also send their outputs to mushroom body output neurons, but this part of the
			network is not included in our mathematical model.

		</p>

		<d-figure>
			<figure id="fig-network-architecture" class="l-body-outset">
				<img src="./img/architecture.001.png" style="width: 100%" class="mb-2" />
				<figcaption>
					Network architecture. Several groups of PNs corresponding to different modalities send
					their activities to the layer of KCs, which are inhibited through the reciprocal connections to the
					APL neuron.
				</figcaption>
			</figure>
		</d-figure>

		<h2 id="blog-training">
			Training the Network
		</h2>


		<p>
			We decomposed each sentence from the training corpus into a collection of $w$-grams of consecutive words.
			The input to the network is a vector that has two blocks: <strong>context</strong> and
			<strong>target</strong>. The target block is a one-hot
			encoding of the middle token in the $w$-gram. The context block encodes the remaining words in the $w$-gram
			as a bag of tokens. An unsupervised learning algorithm<d-cite key="krotovHopfield2019"></d-cite> is used
			for learning the weights of the neural network (see also <a
				href="https://www.ibm.com/blogs/research/2019/04/biological-algorithm/" target="_blank" rel="noopener noreferrer">this blog post</a>). Mapping
			this idea back to the fruit flies that this network is borrowed from, we can think about the context of a
			word as for example an <em>olfactory</em> input to the network of KCs, while the target word
			as for example a <em>visual</em> input. An example of encoding is shown below.
		</p>

		<d-figure>
			<figure class="l-body">
				<img src="./img/ENCODING.003.png" style="width: 100%" />
				<figcaption>
					The encoding method. The input vector consists of two blocks separated by the (thick) blue
					line. Assuming $w = 3$, a center word <span class="token">stock</span> is the target word and the
					two flanking words form a
					context. The $w$-gram is highlighted in light blue.
				</figcaption>
			</figure>
		</d-figure>

		<h2 id="blog-explorer">
			Individual KCs Explorer
		</h2>

		<p>
			In this part of the demo the user can select individual neurons from the group of KCs and explore the
			strength of synaptic weights connecting the selected KC to PNs. Each PN corresponds to a token from our
			vocabulary. The larger the synaptic strength is, the stronger that token contributes to the excitation
			signal to the given KC. For every KC the strengths of incoming synapses from the PNs are passed through a
			softmax function and displayed as a histogram.
		</p>

		<p>
			Consider for example a neuron number 75. The top three tokens activating this KC correspond to the
			words: disease, disorder, patients. It is clear that this neuron has learned a concept associated
			with medical conditions.
		</p>

		<figure>
			<div id="fig-neuron-disease" style="height: 380px;"></div>
		</figure>

		<p>
			Another example, the neuron number 353 has the top tokens: governments, authorities, state, as shown below.
			Thus, it has learned a concept of authorities of various levels. It is clear, however, that this concept is
			not pure. For example, the 5-th strongest token here represents a number-token. Perhaps this is not
			surprising given that a number token can easily appear in the training corpus quantifying, for example, a
			number of law enforcement officers. Another sign of entanglement of concepts for this KC is the token
			<span class="token">news</span>. It is not an authority, but, at the same time, this word can easily appear
			in the
			context of governmental authorities since they are often discussed in the news.
		</p>

		<d-figure>
			<figure>
				<div id="fig-neuron-authority" style="height:380px;"></div>
			</figure>
		</d-figure>

		<!-- <p>
			This demo allows the user to select every individual neuron in this network and inspect the synapses that
			this neuron has learned.
		</p> -->

		<h2 id="blog-query">
			Response to a Query Sentence
		</h2>

		<p>
			In the second part of the demo we explore the activation patterns of neurons in the fruit fly network in
			response to an input sentence. In our model the query sentence is encoded as a bag of words and presented to
			the aforementioned network as the activity of PNs. The recurrent network of KCs coupled to the APL neuron
			performs the calculation and outputs a pattern of excitations across KCs so that only a small number of KCs
			are in the ON state, while the majority of them are in the OFF state. This pattern of excitations is the
			hash code for the query sentence, which is used for the evaluations of our algorithm, see the <a
				href="https://arxiv.org/abs/2101.06887" target="_blank" rel="noopener noreferrer">paper</a>.
		</p>

		<p>
			For a given input sentence our demo returns the word cloud visualizations of words strongly contributing to
			the excitation of four top activated KCs. The synaptic weights learned by these KCs are used to generate the
			probability distributions of individual tokens learned by those KCs by passing the weights through a softmax
			function. These probability distributions are visualized as word clouds.
		</p>

		<p>
			Consider the example sentence:
		</p>

		<blockquote class="">Entertainment industry shares rise following the premiere of the mass
			destruction weapon documentary.</blockquote>

		<d-figure>
			<figure>
				<div id="fig-entertainment-cloud"
					class="grid grid-cols-2 lg:grid-flow-row lg:grid-cols-4 place-items-center" style="column-gap: 0.075rem"></div>
				<figcaption>
					The top results for the word clouds of tokens learned by the four highest activated KCs. In the
					inset of each word cloud, one can see the strength of activation for that specific KC (highlighted
					in red) compared to the activations of the remaining KCs (shown in gray).
				</figcaption>
			</figure>
		</d-figure>

		<p>
			This query sentence activates several KCs. The demo shows the word
			clouds of tokens learned by the four highest-activated KCs. In the inset of each word cloud one can see the
			strength of the activation of that specific KC (highlighted in red) compared to the activations of the
			remaining KCs (shown in gray). The top activated KC has the largest weights for the words: <span
				class="token">weapon</span>, <span class="token">mass</span>, etc. The second-highest activated KC is
			sensitive to the words <span class="token">market</span>, <span class="token">stock</span>,
			etc. This illustrates how the fruit fly network processes the queries. In this example the query refers to
			several distinct combinations of concepts: <span class="token-chunk">weapon of mass destruction</span>,
			<span class="token-chunk">stock market</span>, <span class="token-chunk">movie
				industry</span>. Each of those concepts has a dedicated KC responsible for it. However, the responses
			are
			not perfect. In this case, we would expect to have the KC which is responsible for the <span
				class="token-chunk">movie industry</span>
			concept to have a higher activation than the KC responsible for the types of <span
				class="token-chunk">weapons of mass
				destruction</span>. But, overall, all the concepts picked by the KCs are meaningful and related to the
			query.
		</p>

		<h2>Summary</h2>

		<p>
			In this work we asked the intriguing question whether the core computational algorithm of one of the best
			studied networks in neuroscience &mdash; the network of KCs in the fruit fly brain &mdash; can be repurposed
			for learning word embeddings from text,
			a well defined machine learning task. We have shown that,
			surprisingly, this network can learn correlations between words and their context while producing high
			quality word embeddings. Our fruit fly network generates binary word embeddings &mdash; vectors of
			<code>[0,1]</code> as opposed to continuous vectors like <a
				href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener noreferrer">GloVe</a>
			<d-cite key="pennington2014glove"></d-cite> and <a
				href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener noreferrer">Word2vec</a>,
			<d-cite key="mikolov2013efficient"></d-cite> which is useful from the perspective of
			memory efficiency. The boolean nature of the FlyVec word embeddings also raises interesting questions
			related to interpretability of the learned embeddings. As we have illustrated in this demo, some of the
			individual KCs have learned semantically coherent concepts. For example, we could ask the question, “Does
			the query sentence pertain to a medical condition?”. If the answer is positive, it is likely that the neuron
			75, discussed above, will be in the ON state for that query. Thus, individual KCs may encode in a logical
			way the presence or the absence of certain attributes of the words and the query sentences. We hope to
			systematically investigate this intriguing aspect of interpretability of FlyVec embeddings in the future.
		</p>


	</d-article>

	<d-appendix>
		<d-bibliography src="/bibliography.bib"></d-bibliography>
	</d-appendix>
</body>

</html>